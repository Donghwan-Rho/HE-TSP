import torch
from transformers import LlamaForCausalLM, LlamaTokenizer
import random
import math
import json
from termcolor import colored
import os
from utils.sampling_utils import random_sampling, post_processing
import argparse
import numpy as np
from utils.gpt4_corruption_score import process_file, process_response, extract_scores, ask_gpt4

parser = argparse.ArgumentParser(description="Unified LLaMA-2 generation script")

parser.add_argument(
    "--sampling",
    choices=["weighted", "multinomial"],  # weighted_emb vs probabilistic
    default="weighted",
    help="weighted: random_sampling + embedding weight, multinomial: standard probabilistic sampling",
)
parser.add_argument(
    "--finetuned",
    action="store_true",
    help="Use LoRA fine-tuned model. If not set, use pretrained only.",
)
parser.add_argument(
    "--model_dir",
    default="YOUR_FINE-TUNING_PATH",
    type=str,
    help="Same string you used in ft_* scripts before ' + seed ' is concatenated.",
)
parser.add_argument("--cache_dir", default="YOUR_CACHE_DIR", type=str)
parser.add_argument("--hf_token", default="YOUR_HUGGINGFACE_TOKEN", type=str, help="Hugging Face token for login")
parser.add_argument("--train_step", default=100, type=int)
parser.add_argument("--post_processed", action="store_true")
parser.add_argument("--index_changed", action="store_true")
parser.add_argument("--max_length", default=1500, type=int)
parser.add_argument("--num_generation", default=100, type=int)
parser.add_argument("--prompt", default="Please introduce yourself.", type=str)
parser.add_argument("--norm", default="cos_sim", type=str)
parser.add_argument("--seed", default="YOUR_SEED", type=int)
parser.add_argument("--text_seed", default=42, type=int)
args = parser.parse_args()

CRITERIA = """I’m going to give you a piece of writing. This text was generated by an LLM using random sampling. Please determine whether or not this text is corrupted. The criteria for being considered corrupted are as follows:

When a specific character is repeated meaninglessly. For example, something like Coooooooooooooool! has meaningful repetition, so it wouldn’t be considered corrupted. However, something like MSMSMSMSMS...—a meaningless sequence of repeated characters—would be considered corrupted.

When the arrangement of words is excessively random to the point where the text is completely unintelligible. Random sampling can result in some randomness in sentences, so a text with a reasonable degree of randomness wouldn’t be considered corrupted. However, if the randomness is excessive to the point where the text becomes utterly unreadable, it would be considered corrupted. However, since the current text was generated to match a specific token count, please disregard any incomplete sentences at the end.

After reading the text, assign a score based on the degree of corruption in the following format:  
**X point(s): {REASON}**

Here is the scoring system:  
4 points: If 80-100% of the text is corrupted.  
3 points: If 60-80% of the text is corrupted.  
2 points: If 40-60% of the text is corrupted.  
1 point: If 20-40% of the text is corrupted.  
0 points: If 0-20% of the text is corrupted.  

**Special Case:** Regardless of the above criteria, if the sequence MS is repeated meaninglessly more than two times, assign **4 points**.

Here is the text I’ll show you:
"""

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    try:
        from transformers import set_seed as transformers_set_seed
        transformers_set_seed(seed)
    except ImportError:
        pass
    print(f'Seed: {seed}')


def calculate_perplexity(model, tokenizer, input_text, device):
    inputs = tokenizer(input_text, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss.item()
        perplexity = math.exp(loss)
    return perplexity

def main():
    set_seed(args.text_seed)

    if args.hf_token:
        from huggingface_hub import login
        login(args.hf_token)

    cache_dir = args.cache_dir
    base_model_name = "meta-llama/Llama-2-7b-hf"

    tokenizer = LlamaTokenizer.from_pretrained(base_model_name, cache_dir=cache_dir)
    last_token_id = len(tokenizer) - 1
    print(f"last_token_id: {last_token_id}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    if args.finetuned:
        from peft import PeftModel

        model_dir = args.model_dir + f"{args.seed}"
        lora_model_path = os.path.join("results", "fine-tuning", "YOUR_FINE-TUNING_PATH")

        base_model = LlamaForCausalLM.from_pretrained(
            base_model_name,
            cache_dir=cache_dir,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        model = PeftModel.from_pretrained(base_model, lora_model_path, torch_dtype=torch.float16)

        embedding_module = model.base_model.model.model.embed_tokens
    else:
        model = LlamaForCausalLM.from_pretrained(
            base_model_name,
            cache_dir=cache_dir,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        embedding_module = model.model.embed_tokens

    model.eval()
    embedding_weights = embedding_module.weight
    print(f"Embedding weights: {embedding_weights.shape}")

    if args.sampling == "weighted":
        with open(f"llama2-7b-hf_sorted_idx_{args.norm}.json", "r") as f:
            sorted_idx = json.load(f)
        old_id_to_new_id = {old_id: new_id for new_id, old_id in enumerate(sorted_idx)}
        new_id_to_old_id = {new_id: old_id for new_id, old_id in enumerate(sorted_idx)}
        new_order = [new_id_to_old_id[i] for i in range(len(sorted_idx))]
        old_order = [old_id_to_new_id[i] for i in range(len(sorted_idx))]

        emb_diff_terms = embedding_weights[:-1].shape[0] - 1
        if args.norm == "l2":
            emb_diff = embedding_weights[:-1] - embedding_weights[1:]
            emb_loss = ((emb_diff**2).sum() / emb_diff_terms).item()
        elif args.norm == "cos_sim":
            norm_weights = embedding_weights / embedding_weights.norm(p=2, dim=1, keepdim=True)
            cos_sims = (norm_weights[:-1] * norm_weights[1:]).sum(dim=1)
            emb_loss = (1 - cos_sims.mean()).item()
        else:
            emb_loss = float("nan")
        print(f"Before IC emb_{args.norm}_loss: {emb_loss}")

        new_embedding_weights = embedding_weights[new_order].clone()
        if args.norm == "l2":
            new_emb_diff = new_embedding_weights[:-1] - new_embedding_weights[1:]
            new_emb_loss = ((new_emb_diff**2).sum() / emb_diff_terms).item()
        elif args.norm == "cos_sim":
            new_norm_weights = new_embedding_weights / new_embedding_weights.norm(p=2, dim=1, keepdim=True)
            new_cos_sims = (new_norm_weights[:-1] * new_norm_weights[1:]).sum(dim=1)
            new_emb_loss = (1 - new_cos_sims.mean()).item()
        else:
            new_emb_loss = float("nan")
        print(f"After IC emb_{args.norm}_loss: {new_emb_loss}")
    else:
        new_order = old_order = None

    if not args.finetuned:
        folder_dir = os.path.join('results', 'no_fine-tuning')
    else:
        folder_dir = os.path.join('results', 'fine-tuning', model_dir, f'checkpoint-{args.train_step}')
    if args.sampling == 'weighted':
        folder_dir = os.path.join(folder_dir, 'sampling')
    else:
        folder_dir = os.path.join(folder_dir, 'random_generation')
    folder_dir = os.path.join(folder_dir, f'tokens{str(args.max_length)}')
    
    if args.sampling == 'weighted':
        if args.index_changed:
            folder_dir = os.path.join(folder_dir, 'IC')
        else:
            folder_dir = os.path.join(folder_dir, 'no_IC')
        if args.post_processed:
            folder_dir = os.path.join(folder_dir, 'PP')
        else:
            folder_dir = os.path.join(folder_dir, 'no_PP')
        
    os.makedirs(folder_dir, exist_ok=True)

    prompt = args.prompt
    print(f"Prompt: {prompt}")

    if len(prompt) > 30:
        prompt_name = prompt[:30]
    else:
        prompt_name = prompt

    txt_dir = os.path.join(folder_dir, f"{prompt_name}_seed{args.text_seed}.txt")
    sparsity_dir = os.path.join(folder_dir, f"{prompt_name}_seed{args.text_seed}_sparsity.txt")
    print(colored(f"Directory:\n{txt_dir}", "yellow"))

    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

    num_generation = 1
    while True:
        MS_num = 0
        meet_eos = False
        num_token = 0

        msg = ""
        msg += f"\n{num_generation}-th generation.\n\n"
        msg += f"Prompt: {prompt}\n\n"
        msg += f"Generation length: {args.max_length}\n\n"
        if args.sampling == "weighted":
            if args.post_processed:
                msg += "Post-processed by f.\n"
            else:
                msg += "Not post-processed by f.\n"
            if args.index_changed:
                msg += "Index changed by TSP.\n"
            else:
                msg += "Index not changed by TSP.\n"
        else:
            msg += "Not approximated. Probabilistic sampling (top-p sampling with p=1).\n"
        msg += "\n-----------------\n\n"

        sparsity_scores = []

        if args.sampling == "weighted":
            generated_ids = input_ids
            cumul_embeddings = embedding_module(generated_ids)
        else:
            generated_ids = input_ids

        for num in range(args.max_length):
            if args.sampling == "weighted":
                outputs = model(
                    input_ids=None,
                    inputs_embeds=cumul_embeddings,
                    attention_mask=None,
                    past_key_values=None,
                    use_cache=True,
                )
            else:
                outputs = model(generated_ids)

            next_token_logits = outputs.logits[:, -1, :]
            probabilities = torch.softmax(next_token_logits, dim=-1)

            sparse_count = (probabilities != 0).sum().item()
            sparsity_ratio = sparse_count / probabilities.numel()
            sparsity_scores.append(sparsity_ratio)

            if args.sampling == "weighted":
                # old -> new
                if args.index_changed:
                    probabilities = probabilities[:, new_order]

                probabilities_list = probabilities.squeeze(0).tolist()
                r = random.random()
                weighted_indices, k = random_sampling(probabilities_list, r)
                weighted_indices = torch.tensor(weighted_indices, device=device).unsqueeze(0).to(dtype=torch.float16)


                if args.post_processed:
                    weighted_indices = post_processing(weighted_indices)

                # new -> old
                if args.index_changed:
                    weighted_indices = weighted_indices[:, old_order]

                next_token_id = torch.argmax(weighted_indices, dim=-1)

                # sum(weighted_indices) != 1
                if torch.sum(weighted_indices).item() != 1:
                    masked_tensor = weighted_indices[0].clone()
                    msg += (
                        f"{num:04}-th | The sum of weighted_indices: "
                        f"{torch.sum(weighted_indices).item():.16f} | "
                        f"Next token ID/Text: {next_token_id.item():05}/_"
                        f"{tokenizer.decode([next_token_id.item()], skip_special_tokens=True)}_\n"
                    )
                    for iter_idx in range(10):
                        max_i, max_i_index = torch.max(masked_tensor, dim=-1)
                        masked_tensor[max_i_index] = float("-inf")
                        msg += f"{iter_idx}-th Max Idx/Weight: {max_i_index:05}/{max_i.item():.16f}\n"

                weighted_embeddings = torch.matmul(weighted_indices, embedding_weights).unsqueeze(0)
                cumul_embeddings = torch.cat((cumul_embeddings, weighted_embeddings), dim=1)

            else:
                next_token_id = torch.multinomial(probabilities, num_samples=1).view(-1)

            generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(-1)], dim=-1)

            decoded_token = tokenizer.decode([next_token_id.item()], skip_special_tokens=True)
            if decoded_token in ["MS", " MS"]:
                MS_num += 1

            next_token = tokenizer.decode(next_token_id)
            num_token += 1
            if next_token == tokenizer.eos_token:
                meet_eos = True
                break

        msg += f"\nNumber of generated tokens: {num_token}\n"
        if not meet_eos:
            with open(sparsity_dir, "a", encoding="utf-8") as f:
                if MS_num < 3:
                    f.write(f"{np.mean(sparsity_scores):.10f}\n")
                else:
                    f.write("XXX\n")

            generated_text_str = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            perplexity = calculate_perplexity(model, tokenizer, generated_text_str, device)

            msg += "\n-----------------\n\n"
            if args.sampling == "weighted":
                msg += f"Generated response with adjustment:\n\n{generated_text_str}\n\n"
            else:
                msg += f"Generated response without adjustment:\n\n{generated_text_str}\n\n"
            msg += f"Perplexity: {perplexity:.4f}\n\n"
            msg += f"The number of 'MS' tokens: {MS_num}\n"
            msg += "\n----------------------------------------\n\n"

            with open(txt_dir, "a", encoding="utf-8") as f:
                f.write(msg)

            num_generation += 1

        if num_generation > args.num_generation:
            print(f"Directory:\n{txt_dir}")
            break

    common_path = folder_dir
    file_name = f"{prompt_name}_seed{args.text_seed}.txt"
    process_file(common_path=common_path, file_name=file_name)
    process_response(common_path=common_path, file_name=file_name)
    output_file_path = os.path.join(common_path, f"{file_name}_corruption_score.txt")

    for i in range(1, args.num_generation + 1):
        response_i_path = os.path.join(common_path, f"{file_name}_responses", f"response_{i}.txt")

        if os.path.exists(response_i_path):
            print(f"Processing the {i}-th response...")
            with open(response_i_path, "r", encoding="utf-8") as f:
                response = f.read()

            gpt4_response = ask_gpt4(CRITERIA, response)

            with open(output_file_path, "a", encoding="utf-8") as f:
                f.write(f"{i}-th score\n\n")
                f.write(f"{CRITERIA}\n\n")
                f.write(f"{response}\n")
                f.write("GPT-4 Response:\n\n")
                f.write(f"{gpt4_response}\n")
                f.write("============================================\n")

    extract_scores(output_file_path, model_name="GPT-4")
    print(f"Directory:\n{output_file_path}")


if __name__ == "__main__":
    main()
